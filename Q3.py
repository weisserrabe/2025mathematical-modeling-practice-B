import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import logging
import joblib
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta

# æœºå™¨å­¦ä¹ å¯¼å…¥
from sklearn.model_selection import (
    train_test_split, TimeSeriesSplit, GridSearchCV, 
    RandomizedSearchCV, cross_val_score, learning_curve
)
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import (
    RandomForestRegressor, GradientBoostingRegressor,
    ExtraTreesRegressor, AdaBoostRegressor, VotingRegressor
)
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score
)
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, f_regression, RFE

# ç»Ÿè®¡åˆ†æå¯¼å…¥
from scipy import stats
from scipy.stats import jarque_bera, shapiro

warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('gold_prediction_enhanced.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8')


@dataclass
class EnhancedConfig:
    """å¢å¼ºé…ç½®ç±»"""
    # æ•°æ®å‚æ•°
    data_file: str = 'Bé¢˜é™„ä»¶ï¼šdata.csv'
    target_column: str = 'Close'
    date_column: str = 'Date'
    
    # ç‰¹å¾å·¥ç¨‹å‚æ•°
    ma_periods: List[int] = None
    lag_periods: List[int] = None
    rolling_windows: List[int] = None
    
    # æ¨¡å‹å‚æ•°
    test_size: float = 0.2
    random_state: int = 42
    cv_folds: int = 5
    n_jobs: int = -1
    
    # è¾“å‡ºå‚æ•°
    output_dir: str = 'results/Q3_enhanced'
    save_models: bool = True
    create_visualizations: bool = True
    
    # é«˜çº§å‚æ•°
    enable_hyperparameter_tuning: bool = True
    enable_feature_selection: bool = True
    n_features_to_select: int = 30
    
    def __post_init__(self):
        if self.ma_periods is None:
            self.ma_periods = [5, 10, 20, 50, 200]
        if self.lag_periods is None:
            self.lag_periods = [1, 2, 3, 5, 10, 20]
        if self.rolling_windows is None:
            self.rolling_windows = [5, 10, 20, 60]


class EnhancedGoldPricePredictor:
    """å¢å¼ºç‰ˆé»„é‡‘ä»·æ ¼é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, config: EnhancedConfig):
        self.config = config
        self.data: Optional[pd.DataFrame] = None
        self.features: Optional[pd.DataFrame] = None
        self.target: Optional[pd.Series] = None
        self.feature_names: List[str] = []
        self.models: Dict[str, Any] = {}
        self.results: Dict[str, Dict] = {}
        self.best_model_name: Optional[str] = None
        self.scaler: Optional[Any] = None
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(self.config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        for subdir in ['plots', 'reports', 'data', 'models']:
            (self.output_dir / subdir).mkdir(exist_ok=True)
    
    def load_and_prepare_data(self) -> pd.DataFrame:
        """åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†"""
        logger.info("å¼€å§‹åŠ è½½å’Œå‡†å¤‡æ•°æ®...")
        
        try:
            # åŠ è½½æ•°æ®
            df = pd.read_csv(self.config.data_file)
            logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
            
            # å¢å¼ºæ—¥æœŸè§£æ
            df[self.config.date_column] = self._parse_dates(df[self.config.date_column])
            
            # ç§»é™¤æ—¥æœŸæ— æ•ˆæˆ–ç›®æ ‡å€¼ç¼ºå¤±çš„è¡Œ
            initial_size = len(df)
            df = df.dropna(subset=[self.config.date_column, self.config.target_column])
            logger.info(f"ç§»é™¤äº† {initial_size - len(df)} è¡Œç¼ºå¤±æ•°æ®")
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values(self.config.date_column).reset_index(drop=True)
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            self._data_quality_checks(df)
            
            self.data = df
            logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆ: {len(df)} è¡Œ, æ—¥æœŸèŒƒå›´: {df[self.config.date_column].min()} åˆ° {df[self.config.date_column].max()}")
            
            return df
            
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½é”™è¯¯: {e}")
            raise
    
    def _parse_dates(self, date_series: pd.Series) -> pd.Series:
        """å¢å¼ºæ—¥æœŸè§£æ"""
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        for fmt in ['%m/%d/%Y', '%Y-%m-%d', '%d/%m/%Y']:
            try:
                return pd.to_datetime(date_series, format=fmt)
            except:
                continue
        
        # å›é€€åˆ°è‡ªåŠ¨è§£æ
        return pd.to_datetime(date_series, infer_datetime_format=True, errors='coerce')
    
    def _data_quality_checks(self, df: pd.DataFrame) -> None:
        """æ‰§è¡Œå…¨é¢çš„æ•°æ®è´¨é‡æ£€æŸ¥"""
        logger.info("æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥...")
        
        # æ£€æŸ¥é‡å¤é¡¹
        duplicates = df.duplicated(subset=[self.config.date_column]).sum()
        if duplicates > 0:
            logger.warning(f"å‘ç° {duplicates} ä¸ªé‡å¤æ—¥æœŸ")
        
        # æ£€æŸ¥ç›®æ ‡å˜é‡çš„å¼‚å¸¸å€¼
        target_data = df[self.config.target_column]
        q1, q3 = target_data.quantile([0.25, 0.75])
        iqr = q3 - q1
        outliers = ((target_data < (q1 - 1.5 * iqr)) | (target_data > (q3 + 1.5 * iqr))).sum()
        logger.info(f"ç›®æ ‡å˜é‡ä¸­å‘ç° {outliers} ä¸ªæ½œåœ¨å¼‚å¸¸å€¼")
        
        # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
        skewness = stats.skew(target_data.dropna())
        kurtosis = stats.kurtosis(target_data.dropna())
        logger.info(f"ç›®æ ‡å˜é‡ - ååº¦: {skewness:.3f}, å³°åº¦: {kurtosis:.3f}")
    
    def create_advanced_features(self) -> pd.DataFrame:
        """åˆ›å»ºå…¨é¢çš„ç‰¹å¾é›†"""
        logger.info("åˆ›å»ºé«˜çº§ç‰¹å¾...")
        
        df = self.data.copy()
        
        # åŸºç¡€ä»·æ ¼ç‰¹å¾
        df = self._create_price_features(df)
        
        # æŠ€æœ¯æŒ‡æ ‡
        df = self._create_technical_indicators(df)
        
        # å¸‚åœºå› å­ç‰¹å¾ï¼ˆåŸºäºQ2ç›¸å…³æ€§åˆ†æï¼‰
        df = self._create_market_features(df)
        
        # æ—¶é—´ç‰¹å¾
        df = self._create_time_features(df)
        
        # ç»Ÿè®¡ç‰¹å¾
        df = self._create_statistical_features(df)
        
        # æ»åå’Œæ»šåŠ¨ç‰¹å¾
        df = self._create_lag_rolling_features(df)
        
        # ç§»é™¤åŒ…å«NaNçš„è¡Œ
        df_clean = df.dropna()
        logger.info(f"ç‰¹å¾åˆ›å»ºå®Œæˆã€‚æ¸…æ´æ•°æ®é›†: {df_clean.shape}")
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        feature_cols = [col for col in df_clean.columns 
                       if col not in [self.config.target_column, self.config.date_column]]
        
        self.features = df_clean[feature_cols]
        self.target = df_clean[self.config.target_column]
        self.feature_names = feature_cols
        
        logger.info(f"æ€»ç‰¹å¾æ•°: {len(feature_cols)}")
        
        return df_clean
    
    def _create_price_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºåŸºç¡€ä»·æ ¼ç‰¹å¾"""
        target = self.config.target_column
        
        # ä»·æ ¼å˜åŒ–
        for period in [1, 5, 20]:
            df[f'price_change_{period}d'] = df[target].diff(period)
            df[f'price_pct_change_{period}d'] = df[target].pct_change(period)
        
        # ä»·æ ¼æ¯”ç‡
        if 'High' in df.columns and 'Low' in df.columns:
            df['high_low_ratio'] = df['High'] / df['Low']
        if 'Open' in df.columns:
            df['close_open_ratio'] = df[target] / df['Open']
        
        # ä»·æ ¼ä½ç½®æŒ‡æ ‡
        for period in [10, 20, 50]:
            df[f'price_position_{period}d'] = (
                (df[target] - df[target].rolling(period).min()) / 
                (df[target].rolling(period).max() - df[target].rolling(period).min())
            )
        
        return df
    
    def _create_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæŠ€æœ¯æŒ‡æ ‡"""
        target = self.config.target_column
        
        # ç§»åŠ¨å¹³å‡çº¿
        for period in self.config.ma_periods:
            df[f'sma_{period}'] = df[target].rolling(window=period).mean()
            df[f'ema_{period}'] = df[target].ewm(span=period).mean()
            
            # ä»·æ ¼ä¸ç§»åŠ¨å¹³å‡çº¿çš„å…³ç³»
            df[f'price_sma_ratio_{period}'] = df[target] / df[f'sma_{period}']
            df[f'price_ema_ratio_{period}'] = df[target] / df[f'ema_{period}']
        
        # RSI
        def calculate_rsi(prices, window=14):
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi
        
        df['rsi_14'] = calculate_rsi(df[target])
        df['rsi_7'] = calculate_rsi(df[target], 7)
        df['rsi_21'] = calculate_rsi(df[target], 21)
        
        # MACD
        ema_12 = df[target].ewm(span=12).mean()
        ema_26 = df[target].ewm(span=26).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']
        
        # å¸ƒæ—å¸¦
        for period in [10, 20]:
            sma = df[target].rolling(window=period).mean()
            std = df[target].rolling(window=period).std()
            df[f'bollinger_upper_{period}'] = sma + (std * 2)
            df[f'bollinger_lower_{period}'] = sma - (std * 2)
            df[f'bollinger_width_{period}'] = df[f'bollinger_upper_{period}'] - df[f'bollinger_lower_{period}']
            df[f'bollinger_position_{period}'] = (df[target] - df[f'bollinger_lower_{period}']) / df[f'bollinger_width_{period}']
        
        # éšæœºæŒ‡æ ‡
        if 'High' in df.columns and 'Low' in df.columns:
            for period in [14, 21]:
                lowest_low = df['Low'].rolling(window=period).min()
                highest_high = df['High'].rolling(window=period).max()
                df[f'stoch_k_{period}'] = 100 * (df[target] - lowest_low) / (highest_high - lowest_low)
                df[f'stoch_d_{period}'] = df[f'stoch_k_{period}'].rolling(window=3).mean()
        
        return df
    
    def _create_market_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºå¸‚åœºå› å­ç‰¹å¾ï¼ˆåŸºäºQ2ç›¸å…³æ€§åˆ†æï¼‰"""
        market_factors = [
            'GDX_Close',    # é»„é‡‘çŸ¿ä¸šETF (r=0.9755)
            'PLT_Price',    # é“‚é‡‘ä»·æ ¼ (r=0.7759)
            'USDI_Price',   # ç¾å…ƒæŒ‡æ•° (r=-0.7216)
            'OF_Price',     # åŸæ²¹ä»·æ ¼ (r=0.7107)
            'SP_close',     # æ ‡æ™®500 (r=-0.6843)
            'USO_Close',    # åŸæ²¹ETF (r=0.6357)
            'OS_Price',     # ç™½é“¶ä»·æ ¼ (r=0.6308)
        ]
        
        for factor in market_factors:
            if factor in df.columns:
                # ä¸é»„é‡‘çš„ä»·æ ¼æ¯”ç‡
                df[f'{factor}_gold_ratio'] = df[factor] / df[self.config.target_column]
                
                # ç›¸å¯¹å˜åŒ–
                df[f'{factor}_pct_change_1d'] = df[factor].pct_change()
                df[f'{factor}_pct_change_5d'] = df[factor].pct_change(5)
                
                # æ»šåŠ¨ç›¸å…³æ€§
                for window in [20, 60]:
                    df[f'{factor}_correlation_{window}d'] = (
                        df[factor].rolling(window).corr(df[self.config.target_column])
                    )
                
                # ç›¸å¯¹å¼ºåº¦
                df[f'{factor}_relative_strength'] = (
                    df[factor] / df[factor].rolling(window=20).mean()
                )
        
        return df
    
    def _create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæ—¶é—´ç‰¹å¾"""
        df['year'] = df[self.config.date_column].dt.year
        df['month'] = df[self.config.date_column].dt.month
        df['day_of_week'] = df[self.config.date_column].dt.dayofweek
        df['day_of_year'] = df[self.config.date_column].dt.dayofyear
        df['quarter'] = df[self.config.date_column].dt.quarter
        df['week_of_year'] = df[self.config.date_column].dt.isocalendar().week
        
        # å‘¨æœŸæ€§ç¼–ç 
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
        df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
        df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)
        df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)
        
        # æ˜¯å¦ä¸ºæœˆæœ«/å­£æœ«/å¹´æœ«
        df['is_month_end'] = df[self.config.date_column].dt.is_month_end.astype(int)
        df['is_quarter_end'] = df[self.config.date_column].dt.is_quarter_end.astype(int)
        df['is_year_end'] = df[self.config.date_column].dt.is_year_end.astype(int)
        
        return df
    
    def _create_statistical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºç»Ÿè®¡ç‰¹å¾"""
        target = self.config.target_column
        
        for window in [10, 20, 60]:
            # æ»šåŠ¨ç»Ÿè®¡
            df[f'rolling_mean_{window}'] = df[target].rolling(window).mean()
            df[f'rolling_std_{window}'] = df[target].rolling(window).std()
            df[f'rolling_var_{window}'] = df[target].rolling(window).var()
            df[f'rolling_skew_{window}'] = df[target].rolling(window).skew()
            df[f'rolling_kurt_{window}'] = df[target].rolling(window).kurt()
            
            # æ»šåŠ¨åˆ†ä½æ•°
            df[f'rolling_q25_{window}'] = df[target].rolling(window).quantile(0.25)
            df[f'rolling_q75_{window}'] = df[target].rolling(window).quantile(0.75)
            df[f'rolling_median_{window}'] = df[target].rolling(window).median()
            
            # ä¸æ»šåŠ¨ç»Ÿè®¡çš„è·ç¦»
            df[f'distance_from_mean_{window}'] = df[target] - df[f'rolling_mean_{window}']
            df[f'distance_from_mean_normalized_{window}'] = (
                df[f'distance_from_mean_{window}'] / df[f'rolling_std_{window}']
            )
            
            # æ»šåŠ¨èŒƒå›´
            df[f'rolling_range_{window}'] = (
                df[target].rolling(window).max() - df[target].rolling(window).min()
            )
        
        return df
    
    def _create_lag_rolling_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæ»åå’Œæ»šåŠ¨ç‰¹å¾"""
        target = self.config.target_column
        
        # æ»åç‰¹å¾
        for lag in self.config.lag_periods:
            df[f'{target}_lag_{lag}'] = df[target].shift(lag)
            
            # æ»åå˜åŒ–
            if lag > 1:
                df[f'{target}_lag_change_{lag}'] = df[target] - df[f'{target}_lag_{lag}']
                df[f'{target}_lag_pct_change_{lag}'] = (
                    (df[target] - df[f'{target}_lag_{lag}']) / df[f'{target}_lag_{lag}']
                )
        
        # æ»šåŠ¨çª—å£ç‰¹å¾
        for window in self.config.rolling_windows:
            df[f'{target}_rolling_mean_{window}'] = df[target].rolling(window).mean()
            df[f'{target}_rolling_std_{window}'] = df[target].rolling(window).std()
            df[f'{target}_rolling_min_{window}'] = df[target].rolling(window).min()
            df[f'{target}_rolling_max_{window}'] = df[target].rolling(window).max()
            
            # æ»šåŠ¨è¶‹åŠ¿
            def rolling_trend(series):
                if len(series) < 2:
                    return np.nan
                x = np.arange(len(series))
                y = series.values
                if np.all(np.isnan(y)):
                    return np.nan
                try:
                    slope, _ = np.polyfit(x, y, 1)
                    return slope
                except:
                    return np.nan
            
            df[f'{target}_rolling_trend_{window}'] = (
                df[target].rolling(window).apply(rolling_trend, raw=False)
            )
        
        return df
    
    def feature_selection(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:
        """ç‰¹å¾é€‰æ‹©"""
        if not self.config.enable_feature_selection:
            return X
        
        logger.info(f"æ‰§è¡Œç‰¹å¾é€‰æ‹©ï¼Œä» {X.shape[1]} ä¸ªç‰¹å¾ä¸­é€‰æ‹© {self.config.n_features_to_select} ä¸ª")
        
        # ä½¿ç”¨SelectKBestè¿›è¡Œç‰¹å¾é€‰æ‹©
        selector = SelectKBest(score_func=f_regression, k=self.config.n_features_to_select)
        X_selected = selector.fit_transform(X, y)
        
        # è·å–é€‰ä¸­çš„ç‰¹å¾åç§°
        selected_features = X.columns[selector.get_support()]
        logger.info(f"é€‰ä¸­çš„ç‰¹å¾: {list(selected_features)}")
        
        return pd.DataFrame(X_selected, columns=selected_features, index=X.index)
    
    def create_enhanced_models(self) -> Dict[str, Pipeline]:
        """åˆ›å»ºå¢å¼ºæ¨¡å‹ç®¡é“"""
        logger.info("åˆ›å»ºå¢å¼ºæ¨¡å‹...")
        
        models = {
            'linear_regression': Pipeline([
                ('scaler', StandardScaler()),
                ('model', LinearRegression())
            ]),
            
            'ridge_regression': Pipeline([
                ('scaler', StandardScaler()),
                ('model', Ridge(random_state=self.config.random_state))
            ]),
            
            'lasso_regression': Pipeline([
                ('scaler', StandardScaler()),
                ('model', Lasso(random_state=self.config.random_state, max_iter=2000))
            ]),
            
            'elastic_net': Pipeline([
                ('scaler', StandardScaler()),
                ('model', ElasticNet(random_state=self.config.random_state, max_iter=2000))
            ]),
            
            'random_forest': Pipeline([
                ('scaler', RobustScaler()),
                ('model', RandomForestRegressor(
                    random_state=self.config.random_state,
                    n_jobs=self.config.n_jobs
                ))
            ]),
            
            'extra_trees': Pipeline([
                ('scaler', RobustScaler()),
                ('model', ExtraTreesRegressor(
                    random_state=self.config.random_state,
                    n_jobs=self.config.n_jobs
                ))
            ]),
            
            'gradient_boosting': Pipeline([
                ('scaler', StandardScaler()),
                ('model', GradientBoostingRegressor(random_state=self.config.random_state))
            ]),
            
            'svr_rbf': Pipeline([
                ('scaler', StandardScaler()),
                ('model', SVR(kernel='rbf'))
            ]),
            
            'svr_linear': Pipeline([
                ('scaler', StandardScaler()),
                ('model', SVR(kernel='linear'))
            ]),
            
            'mlp_regressor': Pipeline([
                ('scaler', StandardScaler()),
                ('model', MLPRegressor(
                    random_state=self.config.random_state,
                    max_iter=1000,
                    early_stopping=True,
                    validation_fraction=0.1
                ))
            ])
        }
        
        self.models = models
        return models
    
    def optimize_hyperparameters(self, model_name: str, param_grid: Dict) -> Any:
        """ä¼˜åŒ–è¶…å‚æ•°"""
        if not self.config.enable_hyperparameter_tuning:
            return self.models[model_name]
        
        logger.info(f"ä¸º {model_name} ä¼˜åŒ–è¶…å‚æ•°")
        
        # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=self.config.cv_folds)
        
        # ä½¿ç”¨RandomizedSearchCVæé«˜æ•ˆç‡
        search = RandomizedSearchCV(
            self.models[model_name],
            param_grid,
            cv=tscv,
            scoring='neg_mean_squared_error',
            n_iter=10,  # å‡å°‘è¿­ä»£æ¬¡æ•°ä»¥æé«˜é€Ÿåº¦
            random_state=self.config.random_state,
            n_jobs=self.config.n_jobs,
            verbose=0
        )
        
        search.fit(self.X_train, self.y_train)
        
        logger.info(f"{model_name} æœ€ä½³å‚æ•°: {search.best_params_}")
        return search.best_estimator_
    
    def train_and_evaluate_models(self) -> Dict[str, Dict]:
        """è®­ç»ƒå’Œè¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        logger.info("è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹...")
        
        # åˆ†å‰²æ•°æ®
        self._split_data()
        
        # ç‰¹å¾é€‰æ‹©
        if self.config.enable_feature_selection:
            self.X_train = self.feature_selection(self.X_train, self.y_train)
            # å¯¹æµ‹è¯•é›†åº”ç”¨ç›¸åŒçš„ç‰¹å¾é€‰æ‹©
            self.X_test = self.X_test[self.X_train.columns]
        
        # å®šä¹‰è¶…å‚æ•°ç½‘æ ¼
        param_grids = {
            'random_forest': {
                'model__n_estimators': [50, 100, 200],
                'model__max_depth': [10, 20, None],
                'model__min_samples_split': [2, 5, 10]
            },
            'gradient_boosting': {
                'model__n_estimators': [50, 100, 200],
                'model__learning_rate': [0.05, 0.1, 0.2],
                'model__max_depth': [3, 6, 9]
            },
            'svr_rbf': {
                'model__C': [0.1, 1, 10],
                'model__gamma': ['scale', 'auto']
            },
            'ridge_regression': {
                'model__alpha': [0.1, 1.0, 10.0, 100.0]
            },
            'lasso_regression': {
                'model__alpha': [0.01, 0.1, 1.0, 10.0]
            }
        }
        
        results = {}
        
        for name, model in self.models.items():
            logger.info(f"è®­ç»ƒ {name}...")
            
            try:
                # å¦‚æœæœ‰å‚æ•°ç½‘æ ¼ï¼Œè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
                if name in param_grids and self.config.enable_hyperparameter_tuning:
                    model = self.optimize_hyperparameters(name, param_grids[name])
                
                # è®­ç»ƒæ¨¡å‹
                model.fit(self.X_train, self.y_train)
                
                # é¢„æµ‹
                y_train_pred = model.predict(self.X_train)
                y_test_pred = model.predict(self.X_test)
                
                # è®¡ç®—æŒ‡æ ‡
                metrics = self._calculate_comprehensive_metrics(
                    self.y_train, y_train_pred, self.y_test, y_test_pred
                )
                
                # å­˜å‚¨ç»“æœ
                results[name] = {
                    'model': model,
                    'train_predictions': y_train_pred,
                    'test_predictions': y_test_pred,
                    **metrics
                }
                
                logger.info(f"{name} - æµ‹è¯• RÂ²: {metrics['test_r2']:.4f}, æµ‹è¯• RMSE: {metrics['test_rmse']:.4f}")
                
            except Exception as e:
                logger.error(f"è®­ç»ƒ {name} æ—¶å‡ºé”™: {e}")
                continue
        
        self.results = results
        
        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        if results:
            self.best_model_name = max(results.keys(), key=lambda x: results[x]['test_r2'])
            logger.info(f"æœ€ä½³æ¨¡å‹: {self.best_model_name}")
        
        return results
    
    def _split_data(self) -> None:
        """ä½¿ç”¨æ—¶é—´åºåˆ—æ–¹æ³•åˆ†å‰²æ•°æ®"""
        # æ—¶é—´åºåˆ—åˆ†å‰²
        split_idx = int(len(self.features) * (1 - self.config.test_size))
        
        self.X_train = self.features.iloc[:split_idx].copy()
        self.X_test = self.features.iloc[split_idx:].copy()
        self.y_train = self.target.iloc[:split_idx].copy()
        self.y_test = self.target.iloc[split_idx:].copy()
        
        logger.info(f"æ•°æ®åˆ†å‰² - è®­ç»ƒ: {len(self.X_train)}, æµ‹è¯•: {len(self.X_test)}")
    
    def _calculate_comprehensive_metrics(self, y_train_true, y_train_pred, y_test_true, y_test_pred) -> Dict:
        """è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡"""
        
        def safe_mape(y_true, y_pred):
            """å®‰å…¨è®¡ç®—MAPEï¼Œé¿å…é™¤é›¶é”™è¯¯"""
            mask = y_true != 0
            if mask.sum() == 0:
                return np.inf
            return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        
        metrics = {
            # è®­ç»ƒæŒ‡æ ‡
            'train_mse': mean_squared_error(y_train_true, y_train_pred),
            'train_mae': mean_absolute_error(y_train_true, y_train_pred),
            'train_r2': r2_score(y_train_true, y_train_pred),
            'train_mape': safe_mape(y_train_true, y_train_pred),
            
            # æµ‹è¯•æŒ‡æ ‡
            'test_mse': mean_squared_error(y_test_true, y_test_pred),
            'test_mae': mean_absolute_error(y_test_true, y_test_pred),
            'test_r2': r2_score(y_test_true, y_test_pred),
            'test_mape': safe_mape(y_test_true, y_test_pred),
        }
        
        # è¡ç”ŸæŒ‡æ ‡
        metrics['train_rmse'] = np.sqrt(metrics['train_mse'])
        metrics['test_rmse'] = np.sqrt(metrics['test_mse'])
        metrics['overfitting'] = metrics['train_r2'] - metrics['test_r2']
        
        return metrics
    
    def create_comprehensive_visualizations(self) -> None:
        """åˆ›å»ºå…¨é¢çš„å¯è§†åŒ–"""
        if not self.config.create_visualizations:
            return
            
        logger.info("åˆ›å»ºç»¼åˆå¯è§†åŒ–...")
        
        # è®¾ç½®æ ·å¼
        sns.set_palette("husl")
        
        # 1. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
        self._plot_model_performance_comparison()
        
        # 2. æœ€ä½³æ¨¡å‹åˆ†æ
        self._plot_best_model_analysis()
        
        # 3. ç‰¹å¾é‡è¦æ€§
        self._plot_feature_importance()
        
        # 4. æ®‹å·®åˆ†æ
        self._plot_residual_analysis()
        
        # 5. å­¦ä¹ æ›²çº¿
        self._plot_learning_curves()
        
        # 6. æ—¶é—´åºåˆ—é¢„æµ‹
        self._plot_time_series_prediction()
        
        logger.info(f"å¯è§†åŒ–å·²ä¿å­˜åˆ° {self.output_dir / 'plots'}")
    
    def _plot_model_performance_comparison(self) -> None:
        """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Comprehensive Model Performance Comparison', fontsize=16, fontweight='bold')

        models = list(self.results.keys())
        test_r2 = [self.results[m]['test_r2'] for m in models]
        test_rmse = [self.results[m]['test_rmse'] for m in models]
        test_mape = [self.results[m]['test_mape'] for m in models]
        overfitting = [self.results[m]['overfitting'] for m in models]

        # RÂ²æ¯”è¾ƒ
        axes[0, 0].bar(models, test_r2, alpha=0.7, color='skyblue')
        axes[0, 0].set_title('Test RÂ² Score')
        axes[0, 0].set_ylabel('RÂ² Score')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].grid(True, alpha=0.3)

        # RMSEæ¯”è¾ƒ
        axes[0, 1].bar(models, test_rmse, alpha=0.7, color='lightcoral')
        axes[0, 1].set_title('Test RMSE')
        axes[0, 1].set_ylabel('RMSE')
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].grid(True, alpha=0.3)

        # MAPEæ¯”è¾ƒ
        axes[0, 2].bar(models, test_mape, alpha=0.7, color='lightgreen')
        axes[0, 2].set_title('Test MAPE (%)')
        axes[0, 2].set_ylabel('MAPE (%)')
        axes[0, 2].tick_params(axis='x', rotation=45)
        axes[0, 2].grid(True, alpha=0.3)

        # è¿‡æ‹Ÿåˆåˆ†æ
        colors = ['red' if x > 0.1 else 'orange' if x > 0.05 else 'green' for x in overfitting]
        axes[1, 0].bar(models, overfitting, alpha=0.7, color=colors)
        axes[1, 0].set_title('Overfitting Analysis')
        axes[1, 0].set_ylabel('Train RÂ² - Test RÂ²')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)

        # æ¨¡å‹æ’å
        ranking_data = pd.DataFrame({
            'Model': models,
            'Test_R2': test_r2
        }).sort_values('Test_R2', ascending=True)

        axes[1, 1].barh(ranking_data['Model'], ranking_data['Test_R2'], alpha=0.7, color='purple')
        axes[1, 1].set_title('Model Ranking (by RÂ²)')
        axes[1, 1].set_xlabel('RÂ² Score')
        axes[1, 1].grid(True, alpha=0.3)

        # æ€§èƒ½æ€»ç»“
        best_model = self.results[self.best_model_name]
        summary_text = f"""Best Model: {self.best_model_name}
Test RÂ²: {best_model['test_r2']:.4f}
Test RMSE: ${best_model['test_rmse']:.2f}
Test MAE: ${best_model['test_mae']:.2f}
Test MAPE: {best_model['test_mape']:.2f}%
Overfitting: {best_model['overfitting']:.4f}"""

        axes[1, 2].text(0.1, 0.5, summary_text, transform=axes[1, 2].transAxes,
                        fontsize=12, verticalalignment='center',
                        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        axes[1, 2].set_title('Best Model Summary')
        axes[1, 2].axis('off')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'plots' / '01_model_performance_comparison.png',
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_best_model_analysis(self) -> None:
        """ç»˜åˆ¶æœ€ä½³æ¨¡å‹è¯¦ç»†åˆ†æ"""
        best_model = self.results[self.best_model_name]
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'Best Model Detailed Analysis: {self.best_model_name}', fontsize=16, fontweight='bold')
        
        # è®­ç»ƒé›†ï¼šé¢„æµ‹ vs å®é™…
        axes[0, 0].scatter(self.y_train, best_model['train_predictions'], alpha=0.6, s=20)
        axes[0, 0].plot([self.y_train.min(), self.y_train.max()], 
                       [self.y_train.min(), self.y_train.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('Actual Price')
        axes[0, 0].set_ylabel('Predicted Price')
        axes[0, 0].set_title(f'Train Set (RÂ² = {best_model["train_r2"]:.4f})')
        axes[0, 0].grid(True, alpha=0.3)
        
        # æµ‹è¯•é›†ï¼šé¢„æµ‹ vs å®é™…
        axes[0, 1].scatter(self.y_test, best_model['test_predictions'], alpha=0.6, s=20, color='orange')
        axes[0, 1].plot([self.y_test.min(), self.y_test.max()], 
                       [self.y_test.min(), self.y_test.max()], 'r--', lw=2)
        axes[0, 1].set_xlabel('Actual Price')
        axes[0, 1].set_ylabel('Predicted Price')
        axes[0, 1].set_title(f'Test Set (RÂ² = {best_model["test_r2"]:.4f})')
        axes[0, 1].grid(True, alpha=0.3)
        
        # æ®‹å·®å›¾
        test_residuals = self.y_test - best_model['test_predictions']
        axes[0, 2].scatter(best_model['test_predictions'], test_residuals, alpha=0.6)
        axes[0, 2].axhline(y=0, color='red', linestyle='--')
        axes[0, 2].set_xlabel('Predicted Price')
        axes[0, 2].set_ylabel('Residual')
        axes[0, 2].set_title('Residual Analysis')
        axes[0, 2].grid(True, alpha=0.3)
        
        # æ®‹å·®åˆ†å¸ƒ
        axes[1, 0].hist(test_residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        axes[1, 0].axvline(test_residuals.mean(), color='red', linestyle='--', 
                          label=f'å‡å€¼: {test_residuals.mean():.4f}')
        axes[1, 0].axvline(test_residuals.std(), color='orange', linestyle='--', 
                          label=f'æ ‡å‡†å·®: {test_residuals.std():.4f}')
        axes[1, 0].set_xlabel('Residual')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('Residual Distribution')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Q-Qå›¾
        stats.probplot(test_residuals, dist="norm", plot=axes[1, 1])
        axes[1, 1].set_title('Residual Normality Test (Q-Q Plot)')
        axes[1, 1].grid(True, alpha=0.3)
        
        # ç´¯ç§¯è¯¯å·®
        cumulative_error = np.cumsum(np.abs(test_residuals))
        axes[1, 2].plot(range(len(cumulative_error)), cumulative_error, linewidth=2)
        axes[1, 2].set_xlabel('Test Sample Index')
        axes[1, 2].set_ylabel('Cumulative Absolute Error')
        axes[1, 2].set_title('Cumulative Prediction Error')
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'plots' / '02_best_model_analysis.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_feature_importance(self) -> None:
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        best_model = self.results[self.best_model_name]['model']
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ç‰¹å¾é‡è¦æ€§
        if hasattr(best_model.named_steps['model'], 'feature_importances_'):
            importance = best_model.named_steps['model'].feature_importances_
            
            # åˆ›å»ºé‡è¦æ€§DataFrame
            feature_names = self.X_train.columns if hasattr(self, 'X_train') else self.feature_names
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importance
            }).sort_values('importance', ascending=False).head(20)  # å‰20ä¸ªç‰¹å¾
            
            plt.figure(figsize=(12, 8))
            plt.barh(range(len(importance_df)), importance_df['importance'], color='steelblue', alpha=0.7)
            plt.yticks(range(len(importance_df)), importance_df['feature'])
            plt.xlabel('Feature Importance')
            plt.title(f'Top 20 Feature Importances: {self.best_model_name}')
            plt.gca().invert_yaxis()
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, v in enumerate(importance_df['importance']):
                plt.text(v + 0.001, i, f'{v:.3f}', va='center')
            
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(self.output_dir / 'plots' / '03_feature_importance.png', 
                       dpi=300, bbox_inches='tight')
            plt.close()
        else:
            logger.info(f"æ¨¡å‹ {self.best_model_name} ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
    
    def _plot_residual_analysis(self) -> None:
        """é«˜çº§æ®‹å·®åˆ†æ"""
        best_model = self.results[self.best_model_name]
        residuals = self.y_test - best_model['test_predictions']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Advanced Residual Analysis', fontsize=16, fontweight='bold')
        
        # æ®‹å·®åˆ†å¸ƒä¸æ­£æ€åˆ†å¸ƒå¯¹æ¯”
        axes[0, 0].hist(residuals, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')
        mu, sigma = stats.norm.fit(residuals)
        x = np.linspace(residuals.min(), residuals.max(), 100)
        axes[0, 0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', lw=2, 
                       label=f'Normal(Î¼={mu:.2f}, Ïƒ={sigma:.2f})')
        axes[0, 0].set_title('Residual Distribution')
        axes[0, 0].set_xlabel('Residual')
        axes[0, 0].set_ylabel('Density')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # æ ‡å‡†åŒ–æ®‹å·®
        standardized_residuals = residuals / np.std(residuals)
        axes[0, 1].scatter(best_model['test_predictions'], np.sqrt(np.abs(standardized_residuals)), alpha=0.6)
        axes[0, 1].set_xlabel('Fitted Value')
        axes[0, 1].set_ylabel('âˆš|Standardized Residual|')
        axes[0, 1].set_title('Scale-Location Plot')
        axes[0, 1].grid(True, alpha=0.3)
        
        # æ®‹å·®éšæ—¶é—´å˜åŒ–
        axes[1, 0].plot(range(len(residuals)), residuals, alpha=0.7, linewidth=1)
        axes[1, 0].axhline(y=0, color='red', linestyle='--')
        axes[1, 0].set_xlabel('Time Index')
        axes[1, 0].set_ylabel('Residual')
        axes[1, 0].set_title('Residuals Over Time')
        axes[1, 0].grid(True, alpha=0.3)
        
        # æ®‹å·®è‡ªç›¸å…³ï¼ˆç®€åŒ–ç‰ˆï¼‰
        lags = range(1, min(21, len(residuals)//4))
        autocorr = [np.corrcoef(residuals[:-lag], residuals[lag:])[0,1] for lag in lags]
        axes[1, 1].bar(lags, autocorr, alpha=0.7)
        axes[1, 1].axhline(y=0, color='red', linestyle='-')
        axes[1, 1].axhline(y=0.1, color='red', linestyle='--', alpha=0.5)
        axes[1, 1].axhline(y=-0.1, color='red', linestyle='--', alpha=0.5)
        axes[1, 1].set_xlabel('Lag')
        axes[1, 1].set_ylabel('Autocorrelation')
        axes[1, 1].set_title('Residual Autocorrelation')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'plots' / '04_residual_analysis.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_learning_curves(self) -> None:
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        best_model = self.results[self.best_model_name]['model']
        
        train_sizes, train_scores, val_scores = learning_curve(
            best_model, self.X_train, self.y_train,
            cv=TimeSeriesSplit(n_splits=3),
            train_sizes=np.linspace(0.1, 1.0, 10),
            scoring='r2',
            n_jobs=self.config.n_jobs
        )
        
        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Train Score', color='blue')
        plt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Validation Score', color='red')
        plt.fill_between(train_sizes, np.mean(train_scores, axis=1) - np.std(train_scores, axis=1),
                        np.mean(train_scores, axis=1) + np.std(train_scores, axis=1), alpha=0.1, color='blue')
        plt.fill_between(train_sizes, np.mean(val_scores, axis=1) - np.std(val_scores, axis=1),
                        np.mean(val_scores, axis=1) + np.std(val_scores, axis=1), alpha=0.1, color='red')
        plt.xlabel('Training Set Size')
        plt.ylabel('RÂ² Score')
        plt.title(f'Learning Curve: {self.best_model_name}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'plots' / '05_learning_curves.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_time_series_prediction(self) -> None:
        """ç»˜åˆ¶æ—¶é—´åºåˆ—é¢„æµ‹"""
        best_model = self.results[self.best_model_name]

        # è·å–æ—¥æœŸæ•°æ®
        if hasattr(self, 'data') and self.config.date_column in self.data.columns:
            dates = self.data[self.config.date_column].iloc[-len(self.y_test):]
        else:
            dates = pd.date_range(start='2020-01-01', periods=len(self.y_test), freq='D')

        plt.figure(figsize=(15, 8))

        # ç»˜åˆ¶å®é™…å€¼å’Œé¢„æµ‹å€¼
        plt.plot(dates, self.y_test.values, label='Actual Price', color='blue', linewidth=2)
        plt.plot(dates, best_model['test_predictions'], label='Predicted Price',
                 color='red', linewidth=2, alpha=0.8)

        # æ·»åŠ ç½®ä¿¡åŒºé—´ï¼ˆåŸºäºRMSEï¼‰
        rmse = best_model['test_rmse']
        plt.fill_between(dates,
                         best_model['test_predictions'] - rmse,
                         best_model['test_predictions'] + rmse,
                         alpha=0.2, color='red', label=f'Â±{rmse:.2f} Confidence Interval')

        plt.xlabel('Date')
        plt.ylabel('Gold Price (USD)')
        plt.title(f'Time Series Prediction: {self.best_model_name}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)

        plt.tight_layout()
        plt.savefig(self.output_dir / 'plots' / '06_time_series_prediction.png',
                    dpi=300, bbox_inches='tight')
        plt.close()
    
    def save_comprehensive_results(self) -> None:
        """ä¿å­˜ç»¼åˆç»“æœ"""
        logger.info("ä¿å­˜ç»¼åˆç»“æœ...")
        
        # ä¿å­˜æ¨¡å‹æ€§èƒ½
        performance_df = pd.DataFrame([
            {
                'model': name,
                'train_r2': result['train_r2'],
                'test_r2': result['test_r2'],
                'train_rmse': result['train_rmse'],
                'test_rmse': result['test_rmse'],
                'train_mae': result['train_mae'],
                'test_mae': result['test_mae'],
                'train_mape': result['train_mape'],
                'test_mape': result['test_mape'],
                'overfitting': result['overfitting']
            }
            for name, result in self.results.items()
        ]).sort_values('test_r2', ascending=False)
        
        performance_df.to_csv(self.output_dir / 'data' / 'model_performance.csv', index=False)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        best_model = self.results[self.best_model_name]
        predictions_df = pd.DataFrame({
            'actual': self.y_test.values,
            'predicted': best_model['test_predictions'],
            'residual': self.y_test.values - best_model['test_predictions'],
            'absolute_error': np.abs(self.y_test.values - best_model['test_predictions']),
            'percentage_error': np.abs((self.y_test.values - best_model['test_predictions']) / self.y_test.values * 100)
        })
        predictions_df.to_csv(self.output_dir / 'data' / 'predictions.csv', index=False)
        
        # ä¿å­˜ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if (hasattr(self.results[self.best_model_name]['model'].named_steps['model'], 'feature_importances_')):
            feature_names = self.X_train.columns if hasattr(self, 'X_train') else self.feature_names
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': self.results[self.best_model_name]['model'].named_steps['model'].feature_importances_
            }).sort_values('importance', ascending=False)
            importance_df.to_csv(self.output_dir / 'data' / 'feature_importance.csv', index=False)
        
        # ä¿å­˜æ¨¡å‹
        if self.config.save_models:
            for name, result in self.results.items():
                joblib.dump(result['model'], self.output_dir / 'models' / f'{name}.pkl')
        
        # ä¿å­˜é…ç½®
        with open(self.output_dir / 'config.json', 'w', encoding='utf-8') as f:
            json.dump(asdict(self.config), f, indent=2, ensure_ascii=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {self.output_dir}")
    
    def generate_comprehensive_report(self) -> None:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        logger.info("ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        
        best_model = self.results[self.best_model_name]
        
        # è®¡ç®—ç»Ÿè®¡æ£€éªŒ
        residuals = self.y_test - best_model['test_predictions']
        try:
            jb_stat, jb_p = jarque_bera(residuals)
            sw_stat, sw_p = shapiro(residuals[:min(5000, len(residuals))])
        except:
            jb_stat, jb_p = np.nan, np.nan
            sw_stat, sw_p = np.nan, np.nan
        
        report = f"""# å¢å¼ºç‰ˆé»„é‡‘ä»·æ ¼é¢„æµ‹æ¨¡å‹åˆ†ææŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ‰§è¡Œæ‘˜è¦
- **æ•°æ®é›†**: {len(self.data)} ä¸ªè§‚æµ‹å€¼
- **ç‰¹å¾**: {len(self.feature_names)} ä¸ªå·¥ç¨‹ç‰¹å¾
- **æœ€ä½³æ¨¡å‹**: {self.best_model_name}
- **æµ‹è¯• RÂ²**: {best_model['test_r2']:.4f}
- **æµ‹è¯• RMSE**: ${best_model['test_rmse']:.2f}
- **æµ‹è¯• MAPE**: {best_model['test_mape']:.2f}%

## æ¨¡å‹æ€§èƒ½æ€»ç»“

"""
        
        # æ·»åŠ æ€§èƒ½è¡¨æ ¼
        performance_df = pd.DataFrame([
            {
                'Model': name,
                'Test RÂ²': f"{result['test_r2']:.4f}",
                'Test RMSE': f"${result['test_rmse']:.2f}",
                'Test MAPE': f"{result['test_mape']:.2f}%",
                'Overfitting': f"{result['overfitting']:.4f}"
            }
            for name, result in self.results.items()
        ])
        
        report += performance_df.sort_values('Test RÂ²', ascending=False).to_string(index=False)
        
        report += f"""

## æœ€ä½³æ¨¡å‹åˆ†æ: {self.best_model_name}
- **é¢„æµ‹å‡†ç¡®åº¦**: {(1 - best_model['test_mape']/100)*100:.1f}% å¹³å‡é¢„æµ‹å‡†ç¡®åº¦
- **è¯¯å·®èŒƒå›´**: Â±${best_model['test_rmse']:.2f} (1ä¸ªæ ‡å‡†å·®)
- **è¿‡æ‹Ÿåˆç¨‹åº¦**: {best_model['overfitting']:.4f} ({'ä½' if best_model['overfitting'] < 0.05 else 'ä¸­ç­‰' if best_model['overfitting'] < 0.1 else 'é«˜'})

## ç»Ÿè®¡æ£€éªŒç»“æœ
- **Jarque-Beraæ£€éªŒ**: ç»Ÿè®¡é‡={jb_stat:.4f}, på€¼={jb_p:.4f}
- **Shapiro-Wilkæ£€éªŒ**: ç»Ÿè®¡é‡={sw_stat:.4f}, på€¼={sw_p:.4f}
- **æ®‹å·®æ­£æ€æ€§**: {'æ­£æ€åˆ†å¸ƒ' if sw_p > 0.05 else 'éæ­£æ€åˆ†å¸ƒ'} 

## å•†ä¸šåº”ç”¨å»ºè®®

### æŠ•èµ„ç­–ç•¥
- **é¢„æµ‹æœŸé™**: é€‚åˆ{'çŸ­æœŸ' if best_model['test_r2'] > 0.9 else 'ä¸­æœŸ'}ä»·æ ¼é¢„æµ‹
- **é£é™©ç®¡ç†**: é¢„æœŸé¢„æµ‹è¯¯å·®åœ¨ ${best_model['test_rmse']:.2f} èŒƒå›´å†…
- **äº¤æ˜“ä¿¡å·**: æ¨¡å‹ç½®ä¿¡åº¦{'é«˜' if best_model['test_r2'] > 0.9 else 'ä¸­ç­‰' if best_model['test_r2'] > 0.8 else 'ä½'}

### é£é™©è¯„ä¼°
- **é¢„æµ‹åŒºé—´**: 68%ç½®ä¿¡åŒºé—´ä¸º Â±${best_model['test_rmse']:.2f}
- **æœ€å¤§è¯¯å·®**: {np.max(np.abs(residuals)):.2f}
- **å¹³å‡è¯¯å·®**: ${best_model['test_mae']:.2f}

### æ¨¡å‹ç›‘æ§å»ºè®®
1. **éƒ¨ç½²å»ºè®®**: {'æ¨èéƒ¨ç½²' if best_model['test_r2'] > 0.85 else 'éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›'}
2. **ç›‘æ§é¢‘ç‡**: ç›‘æ§æ¨¡å‹æ€§èƒ½é€€åŒ–
3. **é‡è®­ç»ƒ**: {'æœˆåº¦' if best_model['test_r2'] > 0.9 else 'å‘¨åº¦'}æ¨¡å‹æ›´æ–°å»ºè®®
4. **é£é™©æ§åˆ¶**: å®æ–½é¢„æµ‹ç½®ä¿¡åŒºé—´

## æŠ€æœ¯ç»†èŠ‚

### ç‰¹å¾å·¥ç¨‹
- ç§»åŠ¨å¹³å‡: {len(self.config.ma_periods)} ä¸ªå‘¨æœŸ
- æ»åç‰¹å¾: {len(self.config.lag_periods)} ä¸ªæ»åæœŸ
- æŠ€æœ¯æŒ‡æ ‡: RSI, MACD, å¸ƒæ—å¸¦ç­‰
- å¸‚åœºå› å­: åŸºäºQ2ç›¸å…³æ€§åˆ†æçš„7ä¸ªå…³é”®å› å­

### æ¨¡å‹é…ç½®
- äº¤å‰éªŒè¯: {self.config.cv_folds}æŠ˜æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
- è¶…å‚æ•°ä¼˜åŒ–: {'å¯ç”¨' if self.config.enable_hyperparameter_tuning else 'ç¦ç”¨'}
- ç‰¹å¾é€‰æ‹©: {'å¯ç”¨' if self.config.enable_feature_selection else 'ç¦ç”¨'}

### ç”Ÿæˆæ–‡ä»¶
- model_performance.csv: è¯¦ç»†æ¨¡å‹æ¯”è¾ƒ
- predictions.csv: æµ‹è¯•é›†é¢„æµ‹å’Œè¯¯å·®
- feature_importance.csv: ç‰¹å¾é‡è¦æ€§æ’å
- æ¨¡å‹å›¾è¡¨: ç»¼åˆå¯è§†åŒ–åˆ†æ
- è®­ç»ƒæ¨¡å‹: åºåˆ—åŒ–æ¨¡å‹å¯¹è±¡

---
*ç”±å¢å¼ºç‰ˆé»„é‡‘ä»·æ ¼é¢„æµ‹ç³»ç»Ÿç”Ÿæˆ*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        with open(self.output_dir / 'reports' / 'comprehensive_analysis_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {self.output_dir / 'reports' / 'comprehensive_analysis_report.md'}")


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    logger.info("å¯åŠ¨å¢å¼ºç‰ˆé»„é‡‘ä»·æ ¼é¢„æµ‹åˆ†æ")
    print("=" * 80)
    print("å¢å¼ºç‰ˆé»„é‡‘ä»·æ ¼é¢„æµ‹æ¨¡å‹ - é—®é¢˜3")
    print("=" * 80)
    
    # åˆ›å»ºé…ç½®
    config = EnhancedConfig()
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = EnhancedGoldPricePredictor(config)
    
    try:
        # æ‰§è¡Œåˆ†ææµç¨‹
        print("\n1. åŠ è½½å’Œå‡†å¤‡æ•°æ®...")
        predictor.load_and_prepare_data()
        
        print("2. åˆ›å»ºé«˜çº§ç‰¹å¾...")
        predictor.create_advanced_features()
        
        print("3. åˆ›å»ºå¢å¼ºæ¨¡å‹...")
        predictor.create_enhanced_models()
        
        print("4. è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹...")
        predictor.train_and_evaluate_models()
        
        print("5. åˆ›å»ºå¯è§†åŒ–...")
        predictor.create_comprehensive_visualizations()
        
        print("6. ä¿å­˜ç»“æœ...")
        predictor.save_comprehensive_results()
        
        print("7. ç”ŸæˆæŠ¥å‘Š...")
        predictor.generate_comprehensive_report()
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        best_model = predictor.results[predictor.best_model_name]
        print("\n" + "=" * 80)
        print("åˆ†æå®Œæˆï¼")
        print("=" * 80)
        print(f"ğŸ“Š è®­ç»ƒæ¨¡å‹æ•°é‡: {len(predictor.results)}")
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {predictor.best_model_name}")
        print(f"ğŸ“ˆ æµ‹è¯• RÂ²: {best_model['test_r2']:.4f}")
        print(f"ğŸ’° æµ‹è¯• RMSE: ${best_model['test_rmse']:.2f}")
        print(f"ğŸ“‰ æµ‹è¯• MAE: ${best_model['test_mae']:.2f}")
        print(f"ğŸ¯ æµ‹è¯• MAPE: {best_model['test_mape']:.2f}%")
        print(f"âš–ï¸  è¿‡æ‹Ÿåˆç¨‹åº¦: {best_model['overfitting']:.4f}")
        
        accuracy = (1 - best_model['test_mape']/100) * 100
        print(f"âœ… å¹³å‡é¢„æµ‹å‡†ç¡®åº¦: {accuracy:.1f}%")
        
        print(f"\nğŸ“ ç»“æœä¿å­˜è‡³: {config.output_dir}")
        print("\nğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   â”œâ”€â”€ plots/                    # 6ä¸ªé«˜è´¨é‡å¯è§†åŒ–å›¾è¡¨")
        print("   â”œâ”€â”€ reports/                  # ç»¼åˆåˆ†ææŠ¥å‘Š")
        print("   â”œâ”€â”€ data/                     # æ€§èƒ½æ•°æ®å’Œé¢„æµ‹ç»“æœ")
        print("   â””â”€â”€ models/                   # è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶")
        
        print("\nğŸ‰ é—®é¢˜3å¢å¼ºç‰ˆé¢„æµ‹å»ºæ¨¡å®Œæˆï¼")
        print("=" * 80)
        
    except Exception as e:
        logger.error(f"åˆ†ææµç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()